{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3_Titanic_Yiyan_Wan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wanyi\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "# Algorithms\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.model_selection as cv\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../Titanic Survival/train.csv does not exist: '../Titanic Survival/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9aad26244799>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../Titanic Survival/train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../Titanic Survival/holdout_test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File ../Titanic Survival/train.csv does not exist: '../Titanic Survival/train.csv'"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../Titanic Survival/train.csv\")\n",
    "df_test = pd.read_csv(\"../Titanic Survival/holdout_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_missing_val(df):\n",
    "    data_dict = {}\n",
    "    for col in df.columns:\n",
    "        data_dict[col] = (df[col].isnull().sum()/df.shape[0])*100\n",
    "    return pd.DataFrame.from_dict(data_dict, orient='index', columns=['MissingValueInPercentage'])\n",
    "\n",
    "print(cal_missing_val(df_train))\n",
    "print(cal_missing_val(df_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.info())\n",
    "print(df_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived = 'survived'\n",
    "not_survived = 'not survived'\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\n",
    "\n",
    "women = df_train[df_train['Sex']=='female']\n",
    "ax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\n",
    "ax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\n",
    "ax.legend()\n",
    "ax.set_title('Female')\n",
    "\n",
    "\n",
    "men = df_train[df_train['Sex']=='male']\n",
    "ax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\n",
    "ax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\n",
    "ax.legend()\n",
    "ax.set_title('Male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion: \n",
    "# Male/Femal infants had higher probability to survive than adults\n",
    "# Male more likely to survive in the age of 20 - 40 \n",
    "# Female more likely to survice in the age of 20 - 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FacetGrid = sns.FacetGrid(df_train, row='Embarked', size=4.5, aspect=1.6)\n",
    "FacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\n",
    "FacetGrid.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclution: male more likely (than woman) to survive if he was embarked from C and woman more likely (than man) to survive if she embarked from Q and S.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='Pclass', y='Survived', data = df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclution: higher social class had higher surviving rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Proccesing \"SibSp\" and \"Parch\"\n",
    "* combining the features to show whether this traveller is alone or with some company \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"family\"] = df_train['SibSp'] + df_train['Parch']\n",
    "df_train.loc[df_train['family']> 0,\"not_alone\"] = 1\n",
    "df_train.loc[df_train['family'] == 0, \"not_alone\"]  = 0\n",
    "df_train['not_alone'] = df_train['not_alone'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"family\"] = df_test['SibSp'] + df_test['Parch']\n",
    "df_test.loc[df_test['family']> 0,\"not_alone\"] = 1\n",
    "df_test.loc[df_test['family'] == 0, \"not_alone\"]  = 0\n",
    "df_test['not_alone'] = df_test['not_alone'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Imputting \"Embarked\" by fillna by random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rationale: Only 2 out of 891 missing value (low %) and port embark is not that relevant to survival \n",
    "df_train['Embarked'].fillna(random.choice(['c','q','s']), inplace=True) \n",
    "df_test['Embarked'].fillna(random.choice(['c','q','s']), inplace=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Imputting \"Age\" by fillna with random normal distribution based on age mean and std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df_train[\"Age\"].mean()\n",
    "std = df_train[\"Age\"].std()\n",
    "is_null = df_train[\"Age\"].isnull().sum()\n",
    "random_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
    "age_slice = df_train[\"Age\"].copy()\n",
    "age_slice[np.isnan(age_slice)] = random_age\n",
    "df_train[\"Age\"] = age_slice\n",
    "df_train[\"Age\"] = df_train[\"Age\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df_test[\"Age\"].mean()\n",
    "std = df_test[\"Age\"].std()\n",
    "is_null = df_test[\"Age\"].isnull().sum()\n",
    "random_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
    "age_slice = df_test[\"Age\"].copy()\n",
    "age_slice[np.isnan(age_slice)] = random_age\n",
    "df_test[\"Age\"] = age_slice\n",
    "df_test[\"Age\"] = df_test[\"Age\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fare converting to int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Fare'] = df_train['Fare'].fillna(0)\n",
    "df_train['Fare'] = df_train['Fare'].astype(int)\n",
    "\n",
    "df_test['Fare'] = df_test['Fare'].fillna(0)\n",
    "df_test['Fare'] = df_test['Fare'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. One-hot-encoding for 'Pclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train,pd.get_dummies(df_train['Pclass'])],axis =1)\n",
    "df_test = pd.concat([df_test,pd.get_dummies(df_test['Pclass'])],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.rename(columns = {1:'Class1',2:'Class2',3:'Class3' })\n",
    "df_test = df_test.rename(columns = {1:'Class1',2:'Class2',3:'Class3' })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Label-encoding sex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = {\"male\": 0, \"female\": 1}\n",
    "df_test['Sex'] = df_test['Sex'].map(genders)\n",
    "df_train['Sex'] = df_train['Sex'].map(genders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. One-hot-encoding for 'Embarked'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.get_dummies(df_train, columns =['Embarked']) \n",
    "df_test = pd.get_dummies(df_test, columns =['Embarked']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Drop useless columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dropping the following columns\n",
    "\n",
    "    * 'family','SibSp','Parch' have been converted for not_alone columns \n",
    "    * 'Name' and 'Ticket' and 'PassengerId' have no value on the determination of survivial\n",
    "    * 'Cabin' may have some values but it was highly missing and Pclass would provide the some information related to the social status.  \n",
    "    * 'Pclass' has been imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.columns)\n",
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['Ticket','Parch','SibSp','Cabin','family','Name','Pclass','PassengerId'], axis=1)\n",
    "df_test = df_test.drop(['Ticket','Parch','SibSp','Cabin','family','Name','Pclass','PassengerId'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Final look at the train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Splitting train data into train and validation before applying model to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df_train['Survived']\n",
    "cols=['Sex','Age','Fare','Embarked_C','Embarked_Q','Embarked_S','not_alone','Class1','Class2','Class3']\n",
    "x=df_train[cols]\n",
    "(x_train, x_test, y_train, y_test) = cv.train_test_split(x, y, test_size=.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Applying Model_1: Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train, y_train)\n",
    "y_pred_dt = dt.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1 score of dt: 75.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_pred_dt = dt.predict(x_test)\n",
    "test_set_F1_dt = f1_score(y_test, y_pred_dt,average = 'weighted')\n",
    "print(\"Test set F1 score of dt: {}\".format(round(test_set_F1_dt*100),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV set F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV F1 score: 77.54\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Compute the array containing the 10-folds CV MSEs\n",
    "CV_set_F1_dt = cross_val_score(dt, x_train, y_train, cv=10, scoring='f1_weighted',n_jobs=-1).mean()\n",
    "print('CV F1 score: {}'.format(round(CV_set_F1_dt*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train set F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 score: 96.88\n"
     ]
    }
   ],
   "source": [
    "dt.fit(x_train, y_train)\n",
    "y_pred_dt = dt.predict(x_train)\n",
    "train_set_F1_dt = f1_score(y_train, y_pred_dt,average = 'weighted')\n",
    "print('Train F1 score: {}'.format(round(train_set_F1_dt*100,2))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since CV score < Train score or Test score, so overfitting ---> Tune parameters by decreasing complexity by decreasing max depth and increasing min samples per leaf  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters until CV F1 score  ~ Train F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=3, min_samples_leaf=4, random_state=0)\n",
    "dt.fit(x_train, y_train)\n",
    "y_pred_dt = dt.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV F1 score: 80.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "CV_set_F1_dt = cross_val_score(dt, x_train, y_train, cv=10, \n",
    "                                  scoring='f1_weighted', \n",
    "                                  n_jobs=-1).mean()\n",
    "print('CV F1 score: {}'.format(round(CV_set_F1_dt*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 score: 82.66\n"
     ]
    }
   ],
   "source": [
    "dt.fit(x_train, y_train)\n",
    "y_pred_dt = dt.predict(x_train)\n",
    "train_set_F1_dt = f1_score(y_train, y_pred_dt,average = 'weighted')\n",
    "print('Train F1 score: {}'.format(round(train_set_F1_dt*100,2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1 score of dt: 76.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_pred_dt = dt.predict(x_test)\n",
    "test_set_F1_dt = f1_score(y_test, y_pred_dt,average = 'weighted')\n",
    "print(\"Test set F1 score of dt: {}\".format(round(test_set_F1_dt*100),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.86"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After tunning, the parameters above showcase the model is optimal ie not overfitting (little bit underfitting). \n",
    "# Using the F-score should be better due to we have large amount of negative (ie more survival than dead) unbalanced data\n",
    "round(f1_score(y_test, y_pred_dt, average='weighted')*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Applying Model_2: Random Forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RandomizedSearchCV to find optimal hyperparameters to prevent overfitting and underfitting.\n",
    " * Or we can use the same way under Decision tree to compare both Test Error, Train error and CV error to tune the hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   56.2s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1500 out of 1500 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_sc...\n",
       "                   iid='warn', n_iter=300, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [100, 311, 522, 733,\n",
       "                                                         944, 1155, 1366, 1577,\n",
       "                                                         1788, 2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 300, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "rf_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1366,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 10,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.87"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "round(f1_score(y_test, y_pred_rf, average='weighted')*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Applying Model_3: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wanyi\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(x_train, y_train)\n",
    "y_pred_svm = linear_svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1 score of dt: 70.0\n"
     ]
    }
   ],
   "source": [
    "# Testset f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "y_pred_svm = linear_svc.predict(x_test)\n",
    "test_set_F1_svm = f1_score(y_test, y_pred_svm, average = 'weighted')\n",
    "print(\"Test set F1 score of dt: {}\".format(round(test_set_F1_svm*100),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV F1 score: 71.54\n"
     ]
    }
   ],
   "source": [
    "# CV f1 score\n",
    "CV_F1_SVM = cross_val_score(linear_svc, x_train, y_train, cv=10, \n",
    "                                  scoring='f1_weighted', \n",
    "                                  n_jobs=-1).mean()\n",
    "print(\"CV F1 score: {}\".format(round(CV_F1_SVM*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wanyi\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 score: 78.27\n"
     ]
    }
   ],
   "source": [
    "# Train f1 score\n",
    "linear_svc.fit(x_train, y_train)\n",
    "y_pred_svm = linear_svc.predict(x_train)\n",
    "\n",
    "train_set_F1_dt = f1_score(y_train, y_pred_svm,average = 'weighted')\n",
    "\n",
    "print('Train F1 score: {}'.format(round(train_set_F1_dt*100,2))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since CV F1 score < Train F1 score, so overfitting ---> Tune parameters by decreasing complexity by decreasing max depth and increasing min samples per leaf. We could consider decrease the number of feature variables.However, in this case, we already have limited features. We will decrease complexity as a compensation to elimiate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wanyi\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "linear_svc = LinearSVC(max_iter = 2, random_state = 1)\n",
    "linear_svc.fit(x_train, y_train)\n",
    "y_pred_svm = linear_svc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 score: 52.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wanyi\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Train f1 score\n",
    "linear_svc.fit(x_train, y_train)\n",
    "y_pred_svm = linear_svc.predict(x_train)\n",
    "train_set_F1_dt = f1_score(y_train, y_pred_svm,average = 'weighted')\n",
    "print('Train F1 score: {}'.format(round(train_set_F1_dt*100,2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1 score of dt: 50.0\n"
     ]
    }
   ],
   "source": [
    "# Testset f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "y_pred_svm = linear_svc.predict(x_test)\n",
    "test_set_F1_svm = f1_score(y_test, y_pred_svm,average = 'weighted')\n",
    "print(\"Test set F1 score of dt: {}\".format(round(test_set_F1_svm*100),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV F1 score: 58.8\n"
     ]
    }
   ],
   "source": [
    "# CV f1 score\n",
    "CV_F1_SVM = cross_val_score(linear_svc, x_train, y_train, cv=55, \n",
    "                                  scoring='f1_weighted', \n",
    "                                  n_jobs=-1).mean()\n",
    "\n",
    "print(\"CV F1 score: {}\".format(round(CV_F1_SVM*100,2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.77"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "round(f1_score(y_test, y_pred_svm, average='weighted')*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After tunning, the above result is the best, we can narrow down the difference between CV score and train score\n",
    "* The F1 scores seems much worse than decision tree and random forest. As such, abandon this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Applying Model_4: Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wanyi\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 2.195254015709299, 'penalty': 'l1'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "logreg = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,random_state=0)\n",
    "distributions = dict(C=uniform(loc=0, scale=4),penalty=['l2', 'l1'])\n",
    "logreg_cv = RandomizedSearchCV(logreg, distributions, random_state=0)\n",
    "search = logreg_cv.fit(x_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1 score of LogReg: 62.0\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,C =2.195254015709299, penalty = 'l1' )\n",
    "logreg.fit(x_train, y_train)\n",
    "y_pred_logreg = logreg.predict(x_test)\n",
    "test_set_F1_LogReg = f1_score(y_test, y_pred_logreg,average = 'weighted')\n",
    "print(\"Test set F1 score of LogReg: {}\".format(round(test_set_F1_LogReg*100),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Applying GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from scipy.stats import uniform as sp_randFloat\n",
    "from scipy.stats import randint as sp_randInt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wanyi\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.00598362105537753,\n",
       " 'max_depth': 4,\n",
       " 'n_estimators': 734,\n",
       " 'subsample': 0.5118197974880978}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(max_depth=4,n_estimators=200,random_state=2)\n",
    "parameters = {'learning_rate': sp_randFloat(),\n",
    "              'subsample'    : sp_randFloat(),\n",
    "              'n_estimators' : sp_randInt(100, 1000),\n",
    "              'max_depth'    : sp_randInt(4, 10) }\n",
    "GB_cv = RandomizedSearchCV(estimator=gb, param_distributions = parameters, cv = 2, n_iter = 10, n_jobs=-1)\n",
    "search = GB_cv.fit(x_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1 score of GB: 81.0\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(max_depth=5,n_estimators=276,random_state=2, learning_rate = 0.08475022579896085, subsample = 0.4387677305543283)\n",
    "gb.fit(x_train, y_train)\n",
    "y_pred_gb = gb.predict(x_test)\n",
    "test_set_F1_LogReg = f1_score(y_test, y_pred_gb,average = 'weighted')\n",
    "print(\"Test set F1 score of GB: {}\".format(round(test_set_F1_LogReg*100),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Concluion:\n",
    "* Based on the F1 score above, Randomforest has the highest score.\n",
    "* We will apply randomforest with specific parameters to apply to the holdout_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models pro and cons:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RandomForest - pro as it is simple to understand and interpret and easy to use. It has flexibility to describe non-linear dependencies. No need to standarize or normalize features. On the othe rside, RF introduces further randomization in the training of individual trees, which increase the sample size and cost longer time to process data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision Tree - pro as it able to capture non-linear relationships between features and labels. Don't require feature scaling such as normalization. On the other side, DTs are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree. They are often relatively inaccurate. It usually causes overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logist Regression - pro as it is a widely used technique because it is very efficient, does not require too many computational resources, it’s highly interpretable, it doesn’t require input features to be scaled, it doesn’t require any tuning, it’s easy to regularize, and it outputs well-calibrated predicted probabilities. For cons, Logistic Regression can only predict a categorical outcome. It is also an Algorithm that is known for its vulnerability to overfitting.Another disadvantage of it is that we can’t solve non-linear problems with logistic regression since it’s decision surface is linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SVM - Pros: It works really well with a clear margin of separation. It is effective in high dimensional spaces. For Cons: It doesn't perform well when we have large data set because the required training time is higher.Also it is increasing the dimensions that may not be easy to visualize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final application to Holdout prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['Sex','Age','Fare','Embarked_C','Embarked_Q','Embarked_Q','not_alone','Class1','Class2','Class3']\n",
    "x_holdout = df_test[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_model = GradientBoostingClassifier(max_depth=5,n_estimators=276,random_state=2, learning_rate = 0.08475022579896085, subsample = 0.4387677305543283)\n",
    "final_model = RandomForestClassifier(n_estimators=1577, criterion = \"gini\", max_depth = 30, min_samples_split = 5, bootstrap= True, min_samples_leaf = 2, max_features = \"sqrt\")\n",
    "final_model.fit(x_train, y_train)\n",
    "y_pred_holdout = final_model.predict(x_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(y_pred_holdout)\n",
    "result.to_csv('result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
